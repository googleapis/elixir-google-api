# Copyright 2019 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# NOTE: This file is auto generated by the elixir code generator program.
# Do not edit this file manually.

defmodule GoogleApi.Dataproc.V1.Model.SparkStandaloneAutoscalingConfig do
  @moduledoc """
  Basic autoscaling configurations for Spark Standalone.

  ## Attributes

  *   `gracefulDecommissionTimeout` (*type:* `String.t`, *default:* `nil`) - Required. Timeout for Spark graceful decommissioning of spark workers. Specifies the duration to wait for spark worker to complete spark decomissioning tasks before forcefully removing workers. Only applicable to downscaling operations.Bounds: 0s, 1d.
  *   `scaleDownFactor` (*type:* `float()`, *default:* `nil`) - Required. Fraction of required executors to remove from Spark Serverless clusters. A scale-down factor of 1.0 will result in scaling down so that there are no more executors for the Spark Job.(more aggressive scaling). A scale-down factor closer to 0 will result in a smaller magnitude of scaling donw (less aggressive scaling).Bounds: 0.0, 1.0.
  *   `scaleDownMinWorkerFraction` (*type:* `float()`, *default:* `nil`) - Optional. Minimum scale-down threshold as a fraction of total cluster size before scaling occurs. For example, in a 20-worker cluster, a threshold of 0.1 means the autoscaler must recommend at least a 2 worker scale-down for the cluster to scale. A threshold of 0 means the autoscaler will scale down on any recommended change.Bounds: 0.0, 1.0. Default: 0.0.
  *   `scaleUpFactor` (*type:* `float()`, *default:* `nil`) - Required. Fraction of required workers to add to Spark Standalone clusters. A scale-up factor of 1.0 will result in scaling up so that there are no more required workers for the Spark Job (more aggressive scaling). A scale-up factor closer to 0 will result in a smaller magnitude of scaling up (less aggressive scaling).Bounds: 0.0, 1.0.
  *   `scaleUpMinWorkerFraction` (*type:* `float()`, *default:* `nil`) - Optional. Minimum scale-up threshold as a fraction of total cluster size before scaling occurs. For example, in a 20-worker cluster, a threshold of 0.1 means the autoscaler must recommend at least a 2-worker scale-up for the cluster to scale. A threshold of 0 means the autoscaler will scale up on any recommended change.Bounds: 0.0, 1.0. Default: 0.0.
  """

  use GoogleApi.Gax.ModelBase

  @type t :: %__MODULE__{
          :gracefulDecommissionTimeout => String.t() | nil,
          :scaleDownFactor => float() | nil,
          :scaleDownMinWorkerFraction => float() | nil,
          :scaleUpFactor => float() | nil,
          :scaleUpMinWorkerFraction => float() | nil
        }

  field(:gracefulDecommissionTimeout)
  field(:scaleDownFactor)
  field(:scaleDownMinWorkerFraction)
  field(:scaleUpFactor)
  field(:scaleUpMinWorkerFraction)
end

defimpl Poison.Decoder, for: GoogleApi.Dataproc.V1.Model.SparkStandaloneAutoscalingConfig do
  def decode(value, options) do
    GoogleApi.Dataproc.V1.Model.SparkStandaloneAutoscalingConfig.decode(value, options)
  end
end

defimpl Poison.Encoder, for: GoogleApi.Dataproc.V1.Model.SparkStandaloneAutoscalingConfig do
  def encode(value, options) do
    GoogleApi.Gax.ModelBase.encode(value, options)
  end
end
